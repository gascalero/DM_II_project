## Continuous Variables

- **Date of birth**: Should be converted to a continuous age variable.

- **Healthcare costs**:
    - Contains negative values. Need to interpret the reason behind this. (Could it be related to the 'insurance cost' variable class? Could it be a reimbursement?)
    - Should be scaled due to the magnitude of its range.

- **Mortality rate per 100K**: Contains negative values. Most likely need to impute or change its value.

- **Outliers detected in**:
    - 'Healthcare Costs'
    - 'Incidence Rate per 100K'
    - 'Mortality Rate per 100K'
    - 'Tumor Size (mm)'

## Categorical Variables

- **Marital status**: Should be removed as it is completely empty.

- **Transfusion history**: Should be removed as it only contains null values.

- **Gender**: Contains an additional class 'P'.
    - (Could it be due to an undefined gender or a minority community?)

- **Healthcare Access**: Contains an additional class '?'.
    - Could be due to unknown information due to unique customer factors (e.g., location, rural or urban area?)

- **Urban or Rural**: Contains repeated classes written differently.

- **Diabetes History**: Almost the entire distribution falls into the 'No' class. Does not add much information. Consider removing.

## Feature Engineering

- Is there a correspondence between the classes of 'Insurance cost' and 'Insurance Status'?

- Could 'Obesity BMI' and 'Diet risk' be highly correlated? Both are categorical variables.

---

23/03/2025:

Work Completed Today (Preprocessing):

- Created preprocessing notebook with basic data cleaning steps
- Implemented data splitting strategy (70% training, 15% validation, 15% test)
- Maintained class distribution through stratified sampling
- Dropped problematic columns as identified in EDA:
-'Marital Status' (completely empty)
-'Transfusion History' (only null values)
-'Diabetes History' (low variance - predominantly 'No' class)
- Applied country-to-continent mapping for geographical feature engineering
- Only performed transformations that don't risk data leakage

Expected next steps:

1. Create Preprocessing Pipeline (more details on step 5)
- Implement winsorizing for handling outliers in continuous variables
- Apply RobustScaler for normalizing variables with different scales
- Create proper encoding for categorical variables
- Ensure all transformations are fitted only on training data

2. Feature Engineering
X- Convert 'Date of Birth' to age variable
X- Investigate relationship between 'Insurance Cost' and 'Insurance Status'
X- Analyze potential correlation between 'Obesity BMI' and 'Diet Risk'
X- Clean up 'Urban or Rural' variable by consolidating duplicate classes
- Investigate and handle special values in 'Gender' ('P') and 'Healthcare Access' ('?')

3. Feature Selection
- Apply filter methods for initial feature importance ranking
- Implement embedded methods with suitable models
- Determine optimal feature subset for modeling
- Document rationale for feature inclusion/exclusion

4. Initial Modeling
- Establish baseline models using processed data
- Compare performance across different algorithms
- Identify promising model architectures for further optimization

5. Decisions on Outliers and Scaling

Outlier Treatment:
- Decided to use winsorizing (1% and 99% percentiles) instead of removing outliers
- Rationale: Very few outliers identified (max 25 per variable in 75,000 rows)
- Winsorizing preserves data volume while limiting extreme values' influence
- Special attention to variables with negative values that should be positive

Scaling Approach:
- Selected RobustScaler as the optimal scaling method
- Rationale: Variables have widely different scales (Healthcare Costs vs Incidence Rates)
- RobustScaler uses median and IQR, making it resistant to outliers
- All continuous variables had very low skewness values (all <0.03), confirming no need for log transformations
- Scaling will be applied only after train/val/test split to prevent data leakage

These decisions balance statistical validity with computational efficiency, maintaining data integrity while preparing for optimal model performance.